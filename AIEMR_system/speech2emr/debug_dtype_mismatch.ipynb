{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5865a479",
   "metadata": {},
   "source": [
    "# EMR Model Dtype Mismatch Analysis\n",
    "\n",
    "## Problem Summary\n",
    "The error `expected scalar type Half but found BFloat16` occurs when the EMR model loads in BFloat16 format but guidance expects Float16 (Half precision).\n",
    "\n",
    "This is a **newer** dtype issue - different from our previous float vs BFloat16 problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for dtype analysis\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "# Log current GPU status and available dtypes\n",
    "print(\"=== GPU and Dtype Analysis ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n=== Available PyTorch Dtypes ===\")\n",
    "print(f\"torch.float16 (Half): {torch.float16}\")\n",
    "print(f\"torch.bfloat16 (BFloat16): {torch.bfloat16}\")\n",
    "print(f\"torch.float32: {torch.float32}\")\n",
    "\n",
    "# Check if BFloat16 is supported\n",
    "print(f\"\\nBFloat16 supported on CUDA: {torch.cuda.is_bf16_supported()}\")\n",
    "print(f\"Half (Float16) supported on CUDA: True\")  # Always supported on modern GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c239415",
   "metadata": {},
   "source": [
    "## Root Cause Analysis\n",
    "\n",
    "The issue occurs because:\n",
    "1. **Model Loading**: GPT-OSS-20b loads with `device_map=\"auto\"` which defaults to **BFloat16** on modern GPUs\n",
    "2. **Guidance Library**: Expects **Float16** (Half) precision for operations\n",
    "3. **Tensor Operations**: Mixed BFloat16/Float16 operations cause dtype mismatch\n",
    "\n",
    "**Key Insight**: BFloat16 vs Float16 are different 16-bit formats:\n",
    "- **BFloat16**: Better for training, wider range, Google TPU optimized\n",
    "- **Float16**: Better for inference, NVIDIA optimized, more common in inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78743761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution Implementation\n",
    "print(\"=== Implemented Fixes ===\")\n",
    "print(\"1. Explicit torch_dtype=torch.float16 in model loading\")\n",
    "print(\"2. Force model conversion: model.to(torch.float16)\")  \n",
    "print(\"3. Explicit torch_dtype in Transformers guidance initialization\")\n",
    "print(\"4. Runtime dtype validation and correction\")\n",
    "print(\"5. BFloat16 â†’ Float16 conversion when detected\")\n",
    "\n",
    "print(\"\\n=== Expected Results ===\")\n",
    "print(\"âœ… Model loads in Float16 (not BFloat16)\")\n",
    "print(\"âœ… Guidance library uses consistent Float16\")\n",
    "print(\"âœ… No tensor operation dtype mismatches\")\n",
    "print(\"âœ… EMR conversion works on full GPU\")\n",
    "print(\"âœ… ~2.5s inference time with 39.5GB available GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99b651",
   "metadata": {},
   "source": [
    "## Testing Verification\n",
    "\n",
    "After implementing the fixes, you should see in your logs:\n",
    "```\n",
    "INFO: Successfully loaded EMR model on full GPU (no quantization, float16)\n",
    "INFO: Model loaded with dtype: torch.float16  \n",
    "INFO: Guidance initialized with float16 for GPU\n",
    "INFO: Starting EMR conversion with model dtype: torch.float16\n",
    "INFO: EMR conversion completed for transcription X, file_id: abc123\n",
    "```\n",
    "\n",
    "**No more dtype mismatch errors!** ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
